we are going to use bag of words pipeline. we are considering every word as feature.
we are going to use NLTK

Installation:
pip install nltk

what does it offer? (nltk.org)
1. provide lot of datasets 
	import nltk
	nltk.download()
2. Corpus- A large collection of text
  a) brown corpus
    from nltk.corpus import brown
    It contains data of different cateogories like adventure, editorial, fiction, government, hobbies, science_fiction etc..
data = brown.sents(cateogories = 'fiction')

Bag of Words Pipeline:
Convert your text into numeric data

1) Get the Data/Corpus
2) Tokenisation, Stopword Removal
3) Stemming/ Lemmatization ( to convert differnt forms of verb into original form)
4) Building a Vocab (List of unique words. All these words are club into single list)
5) Vectorization (make a vector like it put those words 1 which are present in the sentence. Size of vector is equal to size of vocab)
6) Classification

Tokenisation:It is the process of dividing text into a set of meaningful pieces. These pieces are called tokens.
from nltk.tokenize import sent_tokenize, word_tokenize
sents = sent_tokenize(document)
print(sents)
print(len(sents))

sentence.split() # it do not split special character
words = word_tokenize(sentence) # it splits special character also

Stopwords:Words which we can skip or of no use to us
from nltk.corpus import stopwords
sw = set(stopwords.words('english'))
print(sw)

def remove_stopwords(text, stopwords):
	useful_words = [w for w in text if w not in stopwords]
	return useful_words

text = " i am not bothered about her very much".split()
useful_text = remove_stopwords(text,sw)
print(useful_text)


Tokenization using Regular Expression:
regexpal.com
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer('[a-zA-Z@.]+')
useful_text = tokenizer.tokenize(sentence)

Stemming:
3 types of stemmer
1) Snowball (multi lingual stemmer) 
2) Porter
3) Lancaster

from nltk.stem.snowball import SnowballStemmer, PorterStemmer
from nltk.stem.lancaster import LancasterStemmer
ps = PorterStemmer()
ps.stem('jumping')
ps.stem('jumps')

# Snowball Stemmer
ss = SnowballStemmer('english')
ss.stem('lovely')
ss.stem('loving')

#Lemmatization (almost same as stemmer)
from nltk.stem import WordNetLemmatizer
wn = WordNetLemmatizer()
wn.lemmatize('jumping')

Building a Vocab and Vectorization:(converting text into numbers)
corpus= [
3 or 4 sentences]
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
vectorized_corpus = cv.fit_transform(corpus)

vectorized_corpus = vectorized_corpus.toarray()
vectorized_corpus[0]
cv.vocabulary_

#Reverse Mapping
# convert numbers into sentences
numbers = vectorized_corpus[2]
numbers
s = cv.inverse_transform(numbers)
print(s)


Vectorization using stopword Removal: (Making your own tokenizer that is modify sentence acc to what you want)
def myTokenizer(document):
	words = tokenizer.tokenize(document.lower())
	# Remove stopwords
	words = remove_stopwords(words, sw)
	return words

myTokenizer(sentence)
print(sentence)
vectorized_corpus = cv.fit_transform(corpus).toarray()


# More ways to create features
1) Unigram - Every word as a feature
2) Bigrams
3) Trigrams
4) n-grams
5) TF-IDF Normalization





